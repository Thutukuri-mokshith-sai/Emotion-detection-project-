{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgwOFBudGkRq",
        "outputId": "ff2408b7-c5bb-49bc-9318-8809aa2495df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     tweet_id   sentiment                                            content\n",
            "0  1956967341       empty  @tiffanylue i know  i was listenin to bad habi...\n",
            "1  1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
            "2  1956967696     sadness                Funeral ceremony...gloomy friday...\n",
            "3  1956967789  enthusiasm               wants to hang out with friends SOON!\n",
            "4  1956968416     neutral  @dannycastillo We want to trade with someone w...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.00      0.00      0.00        19\n",
            "     boredom       0.00      0.00      0.00        31\n",
            "       empty       0.33      0.01      0.01       162\n",
            "  enthusiasm       0.00      0.00      0.00       163\n",
            "         fun       0.04      0.01      0.01       338\n",
            "   happiness       0.32      0.34      0.33      1028\n",
            "        hate       0.49      0.18      0.26       268\n",
            "        love       0.49      0.38      0.43       762\n",
            "     neutral       0.34      0.56      0.42      1740\n",
            "      relief       0.32      0.02      0.04       352\n",
            "     sadness       0.35      0.25      0.29      1046\n",
            "    surprise       0.30      0.04      0.07       425\n",
            "       worry       0.32      0.47      0.38      1666\n",
            "\n",
            "    accuracy                           0.34      8000\n",
            "   macro avg       0.25      0.17      0.17      8000\n",
            "weighted avg       0.33      0.34      0.31      8000\n",
            "\n",
            "Accuracy Score: 0.341625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Install and Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Step 2: Load Dataset\n",
        "data = pd.read_csv(\"/tweet_emotions.csv\")\n",
        "print(data.head())\n",
        "\n",
        "# Step 3: Preprocessing\n",
        "nltk.download('stopwords')\n",
        "stemmer = PorterStemmer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(text):\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text)    # Remove punctuation/numbers\n",
        "    text = text.lower()\n",
        "    words = text.split()\n",
        "    words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "data['clean_text'] = data['content'].apply(preprocess)  # assuming 'content' column has text\n",
        "\n",
        "# Step 4: Train/Test Split\n",
        "X = data['clean_text']\n",
        "y = data['sentiment']   # assuming 'sentiment' column has emotion labels\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Step 6: Model Training\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_vec, y_train)\n",
        "\n",
        "# Step 7: Prediction and Evaluation\n",
        "y_pred = model.predict(X_test_vec)\n",
        "\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to predict emotion for custom input\n",
        "def predict_emotion(text):\n",
        "    # Preprocess the input text\n",
        "    text_clean = preprocess(text)\n",
        "    # Vectorize the input text\n",
        "    text_vec = vectorizer.transform([text_clean])\n",
        "    # Predict the emotion\n",
        "    prediction = model.predict(text_vec)\n",
        "    return prediction[0]\n",
        "\n",
        "# Example Testing\n",
        "sample_texts = [\n",
        "    \"I am feeling so wonderful and happy today!\",\n",
        "    \"This is the worst day of my life. I'm devastated.\",\n",
        "    \"I'm so excited about the concert tonight!\",\n",
        "    \"Why does everything make me so angry lately?\",\n",
        "    \"I feel lonely and sad sometimes.\",\n",
        "    \"That was hilarious! I can't stop laughing.\"\n",
        "]\n",
        "\n",
        "for text in sample_texts:\n",
        "    emotion = predict_emotion(text)\n",
        "    print(f\"Text: {text}\\nPredicted Emotion: {emotion}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FH_h_KVwIf5J",
        "outputId": "1aaa6d32-c3f9-4896-89ba-0d4365021ab9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: I am feeling so wonderful and happy today!\n",
            "Predicted Emotion: happiness\n",
            "\n",
            "Text: This is the worst day of my life. I'm devastated.\n",
            "Predicted Emotion: worry\n",
            "\n",
            "Text: I'm so excited about the concert tonight!\n",
            "Predicted Emotion: happiness\n",
            "\n",
            "Text: Why does everything make me so angry lately?\n",
            "Predicted Emotion: worry\n",
            "\n",
            "Text: I feel lonely and sad sometimes.\n",
            "Predicted Emotion: sadness\n",
            "\n",
            "Text: That was hilarious! I can't stop laughing.\n",
            "Predicted Emotion: happiness\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save the trained model\n",
        "with open('emotion_detection_model.pkl', 'wb') as model_file:\n",
        "    pickle.dump(model, model_file)\n",
        "\n",
        "# Save the TF-IDF vectorizer\n",
        "with open('tfidf_vectorizer.pkl', 'wb') as vec_file:\n",
        "    pickle.dump(vectorizer, vec_file)\n",
        "\n",
        "print(\"Model and vectorizer have been saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGE5Z2JfIpht",
        "outputId": "5e7307d8-f491-4186-e001-bf2d20e3193a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and vectorizer have been saved successfully.\n"
          ]
        }
      ]
    }
  ]
}